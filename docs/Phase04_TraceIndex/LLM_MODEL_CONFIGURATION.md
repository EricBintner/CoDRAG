# CoDRAG LLM & Model Configuration

## Overview

CoDRAG requires multiple AI models for different tasks in a tiered processing pipeline. This document specifies the model configuration system, inspired by [Halley's AI Models settings](../../../LinuxBrain/halley_core/frontend/src/components/SettingsTabs.tsx).

---

## Model Slots

CoDRAG uses **4 model slots** with distinct purposes:

| Slot | Purpose | Default Source | Required |
|------|---------|----------------|----------|
| **Embedding Model** | Vector embeddings for semantic search | `nomic-embed-text` via Ollama or HuggingFace | âœ… Yes |
| **Small Model** | Fast analysis, parsing, tagging | Ollama endpoint (e.g., `qwen3:4b`) | âš ï¸ Recommended |
| **Large Model** | Complex reasoning, summaries, synthesis | Ollama endpoint (e.g., `mistral`, `qwen3:30b`) | âš ï¸ Recommended |
| **CLaRa Model** | Context compression (16x) | `apple/CLaRa-7B-Instruct` via HF or endpoint | âŒ Optional |

### Tiered Processing Strategy

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. EMBEDDING MODEL (nomic-embed-text)                           â”‚
â”‚    - Encode query â†’ vector                                       â”‚
â”‚    - Semantic search over index                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SMALL MODEL (fast, 4B params)                                â”‚
â”‚    - Parse intent                                                â”‚
â”‚    - Quick relevance scoring                                     â”‚
â”‚    - Auto-tagging during index build                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LARGE MODEL (powerful, 7B-30B+ params)                       â”‚
â”‚    - Per-symbol summaries (build-time)                           â”‚
â”‚    - Complex synthesis queries                                   â”‚
â”‚    - "Explain this codebase" style questions                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. CLaRa (optional compression)                                 â”‚
â”‚    - Compress assembled context 16x                              â”‚
â”‚    - Fit more evidence in LLM context window                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model Slot Specifications

### Slot 1: Embedding Model

**Purpose:** Generate vector embeddings for semantic search.

**Configuration Options:**
1. **Ollama Endpoint** (recommended for simplicity)
   - URL: `http://localhost:11434`
   - Model: `nomic-embed-text`
   
2. **HuggingFace Direct Download** (runs in-app with Python)
   - Repo: `nomic-ai/nomic-embed-text-v1.5`
   - One-click download button
   - Managed by CoDRAG (no external server needed)

**UI Pattern:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”¢ Embedding Model                                              â”‚
â”‚ Vector encoding for semantic search                             â”‚
â”‚                                                                 â”‚
â”‚ â—‹ Use Ollama endpoint                                           â”‚
â”‚   Endpoint: [http://localhost:11434    â–¼]                       â”‚
â”‚   Model:    [nomic-embed-text          â–¼] [â†»]                   â”‚
â”‚                                                                 â”‚
â”‚ â—‹ Download from HuggingFace (runs locally)                      â”‚
â”‚   Model: nomic-ai/nomic-embed-text-v1.5                         â”‚
â”‚   Status: â— Downloaded (274MB)          [Re-download]           â”‚
â”‚                                                                 â”‚
â”‚ [Test Connection]                        Active: â— Connected    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API Contract:**
```typescript
interface EmbeddingConfig {
  source: 'ollama' | 'huggingface';
  // Ollama mode
  ollama_endpoint?: string;
  ollama_model?: string;
  // HuggingFace mode
  hf_repo_id?: string;
  hf_model_path?: string;  // Local cache path
}
```

---

### Slot 2: Small Model (Fast Analysis)

**Purpose:** Quick parsing, intent detection, auto-tagging.

**Configuration:**
- Endpoint selector (Ollama, OpenAI-compatible, Claude API)
- Model selector (populated from endpoint)

**Recommended Models:**
- `qwen3:4b-instruct` (Ollama)
- `phi-3-mini` (Ollama)
- `gpt-4o-mini` (OpenAI)

**UI Pattern:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš¡ Small Model                                                   â”‚
â”‚ Fast analysis & parsing                                         â”‚
â”‚                                                                 â”‚
â”‚ Endpoint: [Select endpoint...         â–¼]                        â”‚
â”‚ Model:    [Select model...            â–¼] [â†»]                    â”‚
â”‚                                                                 â”‚
â”‚ Status: â—‹ Not configured                                        â”‚
â”‚                                                                 â”‚
â”‚ [Test Connection]                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Slot 3: Large Model (Complex Reasoning)

**Purpose:** Summaries, synthesis, complex queries.

**Configuration:**
- Endpoint selector (Ollama, OpenAI-compatible, Claude API)
- Model selector (populated from endpoint)

**Recommended Models:**
- `mistral` / `mistral-nemo` (Ollama)
- `qwen3:30b-instruct` (Ollama)
- `deepseek-coder-v2` (Ollama)
- `gpt-4o` (OpenAI)
- `claude-3-5-sonnet` (Anthropic)

**UI Pattern:** Same as Small Model, different slot.

---

### Slot 4: CLaRa (Context Compression)

**Purpose:** 16x context compression for fitting more evidence in prompts.

**Configuration Options:**
1. **HuggingFace Direct Download** (runs in-app)
   - Repo: `apple/CLaRa-7B-Instruct`
   - Requires ~14GB VRAM (fp16) or unified memory
   - One-click download + auto-quantization
   
2. **Remote CLaRa Server** (runs on another machine)
   - URL: `http://192.168.x.x:8765`
   - Leverages existing [CLaRa-Remembers-It-All](../../../CLaRa-Remembers-It-All/) deployment

**UI Pattern:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ—œï¸ CLaRa (Context Compression)                                  â”‚
â”‚ Apple's 16x semantic compression                                â”‚
â”‚                                                                 â”‚
â”‚ â—‹ Download from HuggingFace (runs locally)                      â”‚
â”‚   Model: apple/CLaRa-7B-Instruct                                â”‚
â”‚   Status: â—‹ Not downloaded              [Download ~14GB]        â”‚
â”‚   Requirements: 14GB+ VRAM or unified memory                    â”‚
â”‚                                                                 â”‚
â”‚ â—‹ Use remote CLaRa server                                       â”‚
â”‚   URL: [http://192.168.1.x:8765        ]                        â”‚
â”‚   [Test Connection]  Status: â—‹ Not connected                    â”‚
â”‚                                                                 â”‚
â”‚ â˜ Enable compression (applies to context assembly)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Integration with CLaRa-Remembers-It-All:**
- CoDRAG can embed CLaRa server code directly (same Python dependencies)
- Or connect to standalone CLaRa server via HTTP
- Same API contract: `POST /compress` with `{memories: string[], query: string}`

---

## Endpoint Configuration

### Saved Endpoints

Users can save multiple endpoints for reuse across model slots.

**Supported Provider Types:**
| Provider | URL Pattern | Auth | Notes |
|----------|-------------|------|-------|
| `ollama` | `http://localhost:11434` | None | Local Ollama server |
| `openai` | `https://api.openai.com/v1` | API Key | OpenAI models |
| `openai-compatible` | Custom URL | API Key | LocalAI, vLLM, etc. |
| `anthropic` | `https://api.anthropic.com` | API Key | Claude models (BYOK) |

**UI Pattern:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”Œ Saved Endpoints                                              â”‚
â”‚ Add endpoints for local or remote LLM servers                   â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ GPU Server (Ollama)                                         â”‚ â”‚
â”‚ â”‚ http://192.168.1.100:11434                    [Edit] [ğŸ—‘ï¸]   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ OpenAI                                                      â”‚ â”‚
â”‚ â”‚ https://api.openai.com/v1                    [Edit] [ğŸ—‘ï¸]   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚ + Add Endpoint                                                  â”‚
â”‚                                                                 â”‚
â”‚ Display Name:   [Local GPU Server            ]                  â”‚
â”‚ Provider Type:  [Ollama                    â–¼]                   â”‚
â”‚ Endpoint URL:   [http://localhost:11434      ]                  â”‚
â”‚ API Key:        [â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢] (if needed)      â”‚
â”‚                                                                 â”‚
â”‚ [Test Connection]  [Save Endpoint]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Model

### Config Schema

```typescript
interface LLMConfig {
  // Embedding model
  embedding: {
    source: 'ollama' | 'huggingface';
    ollama_endpoint?: string;
    ollama_model?: string;
    hf_repo_id?: string;
    hf_downloaded?: boolean;
    hf_model_path?: string;
  };
  
  // Small model (fast)
  small_model: {
    enabled: boolean;
    endpoint_id?: string;  // Reference to saved endpoint
    model?: string;
  };
  
  // Large model (powerful)
  large_model: {
    enabled: boolean;
    endpoint_id?: string;
    model?: string;
  };
  
  // CLaRa compression
  clara: {
    enabled: boolean;
    source: 'huggingface' | 'remote';
    hf_downloaded?: boolean;
    hf_model_path?: string;
    remote_url?: string;
  };
  
  // Saved endpoints
  saved_endpoints: Array<{
    id: string;
    name: string;
    provider: 'ollama' | 'openai' | 'openai-compatible' | 'anthropic';
    url: string;
    api_key?: string;  // Encrypted at rest
  }>;
}
```

### Storage Location

```
~/.local/share/codrag/
â”œâ”€â”€ config.yaml           # Global config including LLM settings
â”œâ”€â”€ models/               # Downloaded HF models
â”‚   â”œâ”€â”€ nomic-embed-text/
â”‚   â””â”€â”€ clara-7b/
â””â”€â”€ ...
```

---

## Backend API

### Endpoints

```
GET  /llm/config                    Get current LLM configuration
POST /llm/config                    Update LLM configuration

GET  /llm/endpoints                 List saved endpoints
POST /llm/endpoints                 Add new endpoint
PUT  /llm/endpoints/{id}            Update endpoint
DELETE /llm/endpoints/{id}          Delete endpoint

POST /llm/endpoints/{id}/test       Test endpoint connection
GET  /llm/endpoints/{id}/models     List available models at endpoint

POST /llm/embedding/test            Test embedding model
POST /llm/small/test                Test small model
POST /llm/large/test                Test large model
POST /llm/clara/test                Test CLaRa connection

POST /llm/hf/download               Start HuggingFace model download
GET  /llm/hf/download/status        Get download progress
POST /llm/hf/delete                 Delete downloaded model
```

### HuggingFace Download Flow

```
User clicks [Download]
    â”‚
    â–¼
POST /llm/hf/download
{
  "model_type": "embedding" | "clara",
  "repo_id": "nomic-ai/nomic-embed-text-v1.5"
}
    â”‚
    â–¼
Server starts background download
Returns: { "download_id": "abc123" }
    â”‚
    â–¼
Frontend polls GET /llm/hf/download/status?id=abc123
Returns: { "progress": 0.45, "status": "downloading", "bytes_downloaded": "1.2GB" }
    â”‚
    â–¼
When complete:
{ "progress": 1.0, "status": "complete", "model_path": "~/.local/share/codrag/models/nomic-embed-text" }
```

---

## UI Implementation

### Settings Page Structure

```
Settings
â”œâ”€â”€ General
â”œâ”€â”€ Projects
â”œâ”€â”€ AI Models  â—€â”€â”€ NEW TAB
â”‚   â”œâ”€â”€ Embedding Model card
â”‚   â”œâ”€â”€ Small Model card
â”‚   â”œâ”€â”€ Large Model card
â”‚   â”œâ”€â”€ CLaRa card
â”‚   â””â”€â”€ Saved Endpoints section
â””â”€â”€ Advanced
```

### Component Hierarchy

```
AIModelsSettings
â”œâ”€â”€ ModelCard (reusable)
â”‚   â”œâ”€â”€ EndpointSelector
â”‚   â”œâ”€â”€ ModelSelector
â”‚   â”œâ”€â”€ HFDownloadButton (optional)
â”‚   â”œâ”€â”€ TestConnectionButton
â”‚   â””â”€â”€ StatusBadge
â”œâ”€â”€ ClaraCard (specialized)
â”‚   â”œâ”€â”€ HFDownloadSection
â”‚   â”œâ”€â”€ RemoteServerSection
â”‚   â””â”€â”€ EnableToggle
â””â”€â”€ SavedEndpointsSection
    â”œâ”€â”€ EndpointList
    â””â”€â”€ AddEndpointForm
```

### Reusable Components

**ModelCard Props:**
```typescript
interface ModelCardProps {
  title: string;
  description: string;
  icon: LucideIcon;
  
  // Endpoint mode
  endpoint?: string;
  endpointOptions: Endpoint[];
  onEndpointChange: (endpoint: string) => void;
  
  // Model selection
  model?: string;
  modelOptions: string[];
  onModelChange: (model: string) => void;
  onRefreshModels: () => void;
  loadingModels?: boolean;
  
  // HuggingFace mode (optional)
  hfDownloadEnabled?: boolean;
  hfRepoId?: string;
  hfDownloaded?: boolean;
  hfDownloadProgress?: number;
  onHFDownload?: () => void;
  
  // Status
  status: 'connected' | 'disconnected' | 'not-configured';
  onTest: () => void;
  testResult?: { success: boolean; message: string };
}
```

---

## Open Questions

1. **API Key Storage:** Encrypt at rest? Use system keychain?
2. **Model Download Location:** Global or per-project?
3. **Ollama Auto-Detect:** Should we auto-discover Ollama at localhost:11434?
4. **Default Models:** Should onboarding pre-select recommended models?

---

## Implementation Priority

| Priority | Task |
|----------|------|
| P0 | Embedding model config (required for core function) |
| P1 | Endpoint management UI |
| P1 | Small/Large model config |
| P2 | HuggingFace download for embeddings |
| P2 | CLaRa integration |
| P3 | Claude/OpenAI BYOK support |

---

## Related Documents

- `README.md` â€” Phase 04 overview
- `TRACEABILITY_AUTOMATION_STRATEGY.md` â€” How LLMs are used in trace augmentation
- `../../ARCHITECTURE.md` â€” Overall CoDRAG architecture
- `../../../CLaRa-Remembers-It-All/README.md` â€” CLaRa server reference implementation
